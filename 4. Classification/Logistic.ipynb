{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_order.pkl', 'rb') as data:\n",
    "    order = pickle.load(data)\n",
    "with open('data_reason.pkl', 'rb') as data:\n",
    "    reason = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contents = []\n",
    "for k, v in dict.items(reason):\n",
    "    contents2 = []\n",
    "    for i in v:\n",
    "        if \"Noun\" in i:\n",
    "            contents2.append(i[0])\n",
    "    contents.append(\" \".join(contents2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'약속어음 할인 교부 수탁 위탁 취지 보관 위탁 약속어음 수탁 자신 채무 변제 충당 수탁 위탁 취지 횡령죄 구성 대법원 참조 채택 종합 무수 어음 번호 자가 액면 발행 박광 일인 약속어음 김성동 할인 요청 이전 박광 일이 발행 부도 약속어음 이자 합계 공제 금액 할인 달라 의뢰 계좌 번호 김성동 약속어음 교부 약속어음 배서 확인 후박광일 약속어음 채권 별도 약속어음 채권 금액 공제 나머지 금액 할인 약속어음 반환 요구 약속어음 채무 변제 약속어음 반환 계속 보관 까지 변제 약속어음 처분 최고 남병삼 약속어음 할인 융통 하라 다음 여러 사정 약속어음 불법 영득 의사 횡령죄 구성 바위 법리 채증 법칙 배하 횡령죄 불법영득의사 법리 오해 기각 관여 법관 일치 의견 주문 대법관 윤재식 재판 변재 강신욱 고현철 주심'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf-idf 벡터 만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "kor_vectorizer = CountVectorizer(min_df=1) # 등장하는 단어들에 대한 오브젝트\n",
    "kor_bow = kor_vectorizer.fit_transform(contents) # 딕셔너리에 실제 단어들을 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in BoW = 57737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of words in BoW = {}\".format(len(kor_vectorizer.get_feature_names())))\n",
    "kor_bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 생성한 TF-bow를 가지고 tf-idf 생성하기\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer() # tfidf 변환 인스턴스 생성\n",
    "tfidf = transformer.fit_transform(kor_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "data_dict['reasons'] = contents\n",
    "data_dict['bow'] = kor_bow\n",
    "data_dict['tfidf'] = tfidf\n",
    "data_dict['vectorizer'] = kor_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"processed_news.obj\", 'wb') as f:\n",
    "    pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"processed_news.obj\", 'rb') as f:\n",
    "    load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['reasons', 'bow', 'tfidf', 'vectorizer'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요지 서울 광진구 자양 소재 머릿돌 교회 신축 공사 현장 총괄 책임자 공사 현장 주차장 정화조 맨홀 직경 깊이 입구 공사 작업 인부 오랑 지시 오랑 장소 발생 안전 사고 예방 맨홀 주위 펜스 설치 외부 출입 통제 뚜껑 맨홀 구멍 견고 조치 합판 확인 점검 시정 지시 공사 감독 로서 업무 주의 의무 합판 박도현 무게 이기 합판 틈새 정화조 추락 사망 다음 사정 유죄 합리 의심 여지 증명 유죄 파기 무죄 정화조 맨홀 공사 직접 실시 증인 이오 지시 정화조 맨홀 두께 원형 합판 길이 두께 정사각형 합판 다시 각목 우물 정자 모양 안전 조치 당시 머릿돌 교회 관리 집사 증인 신현오 역시 자신 실종 당일 퇴근 안전 조치 증언 바위 오랑 신현오 모두 머릿돌 교회 관계 그대로 신고 현장 출동 정화조 현장 정화조 맨홀 구멍 바로 반장 정도 합판 증인 주병철 각형 합판 정화조 구멍 안쪽 형태 취지 증인 박기종 증인 증언 종합 지시 정화조 맨홀 공사 시공 오랑 합판 최소한 사각형 합판 안전 조치 취한 여지 성불 준영 아이 준영 이의 목격 이후 발견 사람 무도 평소 이후 귀가 전혀 일응 저녁 무렵 실종 실종 당일 정화조 맨홀 공사 담당 오랑 공사 완료 정화조 구멍 합판 이후 정화조 부근 조금 콘크리트 타설 작업 실종 무렵 정화조 부근 콘크리트 타설 작업 인부 발견 거기 아버지 박기종 친척 실종 당일 정화조 부근 이상 발견 하루 가까이 경과 저녁 무렵 정화조 상태 의심 정화조 수색 경위 실종 당일 이후 정화조 추락 합리 나이 귀가 습성 감안 이후 정화조 추락 보기 사인 경찰 검시 조서 의사 문상 사체 검안 신체 외상 흔적 사망 정화조 실족 내부 익사 추정 사체 검안 의사 문상 정화조 직후 사체 검안 사망원인 미상 고려 정화조 발견 근거 정화조 고인 익사 추정 것일 외관 의사 사망원인 상태 실종 사체 발견 하루 정도 간격 앞서 실종 경위 석연치 고려 실제 익사 가능성 다른 원인 사망 정화조 유기 가능성 배제 다음 수긍 형사재판 증명력 법관 자유 논리 경험칙 여야 형사재판 유죄 심증 형성 정도 합리 의심 여지 정도 하나 합리 모든 의심 배제 정도 요구 증명력 합리 근거 의심 배척 자유 심증 주의 한계 허용 또한 증명 반드시 직접 논리 경험칙 합치 간접 간접 개별 증명력 가지 전체 상호 관련하 종합 고찰 단독 가지 종합 증명력 수가 대법원 참조 우선 안전 조치 이행 여부 당부 증인 이오 신현오 스스로 시한 그대로 증인 주병철 시체 발견 당시 현장 정화조 맨홀 구멍 바로 정도 합판 증인 오랑 달리 정도 무게 맨홀 뚜껑 근처 각목 맨홀 구멍 안쪽 합판 매우 증인 박기종 역시 각형 합판 정화조 구멍 안쪽 형태 합판 모양 원형 정화조 주위 별도 합판 각목 증인 인의 종합 보아 안전 조치 두께 합판 두께 정방형 합판 우물 정자 모양 배치 토막 각목 실제 보기 오히려 오랑 맨홀 입구 콘크리트 타설 그것 상태 맨홀 뚜껑 대신 합판 장만 개연 매우 더구나 인근 주민 이봉 학박 종례 경찰 공사 현장 인근 거주 아이 접근 통제 울타리 차단 시설 전혀 따라서 이행 안전 치가 실제 이행 가능성 배척 그릇 사실관계 기초 합리 근거 다음 사망 익사 이외 것일 가능성 배제 또한 합리 근거 실종 당일 정화조 부근 조금 콘크리트 타설 작업 제하 작업 종료 맨홀 정화조 추락 단정 사고 당일 이후 측구 콘크리트 타설 공사 뒷받침 객관 자료 작업 일지 수사 작업 일지 내지 행간 간격 비교 측구 콘크리트 타설 공사 투입 인원 도로 직전 직후 부자연 간격 마지막 당일 투입 인원 페이지 다른 필체 가필 의심 공사 투입 인원 나머지 인원 수의 합계 이오 측구 공사 경찰 법정 언급 오히려 자신 때문 이후 인근 거주 아이 공사 현장 접근 통제 방법 취지 경찰 피의자신문 비로소 사고 당일 측구 공사 직후 작업 일지 사본 경찰 제출 보면측구 콘크리트 타설 공사 관련 작업 일지 사후 추가 기입 공사 마지막 정화조 맨홀 높이 수정 청소 까지 현장 작업 사고 직후 오랑 경찰 맨홀 입구 공사 사고 당일 완료 제출 작업 일지 통틀어 보아 작업 종료 시간 발견 까지 현장 작업 역시 사후 가필 작업 일지 달리 사고 당일 평소 까지 측구 콘크리트 타설 공사 사정 가족 실종 당일 정화조 부근 이상 발견 근거 자기 정화조 추락 시각 실종 당일 이후 보아 하나 나이 평소 귀가 귀가 습성 감안 이후 정화조 추락 보기 사고 당일 심야 정화조 상태 확인 증인 신현오 증언 앞서 아버지 박기종 사고 당일 정화조 근처 모랫 더미 근방 다시 당시 정화조 부근 정화조 조사해 보지 더구나 앞서 까지 공사 현장 인부 작업 계속 보기 이상 사정 정화조 추락 시점 사고 당일 이후 추정 근거 박약 수중 시체 익사 사후 투수 인지 판별 부검 원칙 하나 타살 다른 원인 사고사 의심 상황 라면 외부 소견 익사 추정 의사 사체 검안 증명력 배척 다른 원인 사망 유기 흔적 유괴 살인 거나 교통사고 사고사 외상 전혀 앞서 실종 시각 경위 의심 합리 근거 현재 로서 사인 의사 문상 사체 검안 증명력 배척 합리 보기 거론 사정 의사 사체 검안 사망원인 미상 어차피 사인 추정 사인 단정 실종 사체 발견 하루 정도 시간 경과 정도 사정 사체 검안 증명력 탄핵 따라서 사체 검안 의사 사망원인 익사 경위 사고 당일 실제 작업 종료 시각 시체 발견 당시 상황 주병철 경찰 시체 인양 당시 상황 촬영 비디오테이프 시중 통용 건축 합판 규격 강도 좀더 심리 결과 기존 제출 사실관계 종합 유죄 여부 제반 종합 증명력 것임 불구 앞서 유죄 여러 개별 가치 예외 현상 증명력 모두 배척 무죄 조치 심리 채증 법칙 결과 영향 따라서 지적 파기 법원 환송 주문 대법관 이용우 재판 무제 이규홍 박재윤 주심'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load['reasons'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 14s, sys: 36.3 s, total: 20min 50s\n",
      "Wall time: 28min 25s\n"
     ]
    }
   ],
   "source": [
    "# clustering\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 19\n",
    "%time kmeans = KMeans(n_clusters=n_clusters, max_iter=300, tol=0.0001).fit(load['tfidf'].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cluster.k_means_.KMeans"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_index_dict = dict()\n",
    "for c_id in range(n_clusters):\n",
    "    cluster_index_dict[c_id] = [i for i, ele in enumerate(kmeans.labels_) if ele == c_id ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster id : 0 - 38\n",
      "cluster id : 1 - 716\n",
      "cluster id : 2 - 167\n",
      "cluster id : 3 - 250\n",
      "cluster id : 4 - 150\n",
      "cluster id : 5 - 906\n",
      "cluster id : 6 - 226\n",
      "cluster id : 7 - 158\n",
      "cluster id : 8 - 591\n",
      "cluster id : 9 - 401\n",
      "cluster id : 10 - 328\n",
      "cluster id : 11 - 713\n",
      "cluster id : 12 - 272\n",
      "cluster id : 13 - 385\n",
      "cluster id : 14 - 152\n",
      "cluster id : 15 - 101\n",
      "cluster id : 16 - 873\n",
      "cluster id : 17 - 570\n",
      "cluster id : 18 - 3171\n"
     ]
    }
   ],
   "source": [
    "for c_id in range(n_clusters):\n",
    "    print(\"cluster id : {} - {}\".format(c_id, len(cluster_index_dict[c_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict['kmeans'] = kmeans\n",
    "data_dict['cluster_index_dict'] = cluster_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "topics = dict()\n",
    "kor_vectorizer = load['vectorizer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "1 [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "2 [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "3 [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "4 [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "5 [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "6 [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "7 [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18]\n",
      "8 [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18]\n",
      "9 [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18]\n",
      "10 [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18]\n",
      "11 [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18]\n",
      "12 [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18]\n",
      "13 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18]\n",
      "14 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18]\n",
      "15 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18]\n",
      "16 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18]\n",
      "17 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18]\n",
      "18 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for target_id in range(n_clusters):\n",
    "# for target_id in [0]:\n",
    "    reference_id_list = np.delete(np.arange(n_clusters), target_id)\n",
    "    target_topic = dict()\n",
    "    print(target_id, reference_id_list)\n",
    "    \n",
    "    for reference_id in reference_id_list:\n",
    "        n_target = len(load['tfidf'].toarray()[cluster_index_dict[target_id]])\n",
    "        n_reference = len(load['tfidf'].toarray()[cluster_index_dict[reference_id]])\n",
    "\n",
    "        target_x = load['tfidf'].toarray()[cluster_index_dict[target_id]]\n",
    "        reference_x = load['tfidf'].toarray()[cluster_index_dict[reference_id]]\n",
    "\n",
    "        target_y = np.ones(n_target)\n",
    "        reference_y = np.zeros(n_reference)\n",
    "\n",
    "        X = np.concatenate([target_x, reference_x])\n",
    "        y = np.concatenate([target_y, reference_y])\n",
    "\n",
    "        model = LogisticRegression(penalty='l1', tol=0.0001, C=15)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        coef_index = [i for i, coef in enumerate(model.coef_[0]) if coef > 0]\n",
    "        for coef in coef_index:\n",
    "            if kor_vectorizer.get_feature_names()[coef] not in target_topic.keys():\n",
    "                target_topic[kor_vectorizer.get_feature_names()[coef]] = model.coef_[0][coef]\n",
    "            else:\n",
    "                if target_topic[kor_vectorizer.get_feature_names()[coef]] < model.coef_[0][coef]:\n",
    "                    target_topic[kor_vectorizer.get_feature_names()[coef]] = model.coef_[0][coef]\n",
    "                \n",
    "#     topics[target_id] = set(target_topic)\n",
    "    topics[target_id] = dict()\n",
    "    topics[target_id]['topic'] = sorted(target_topic.keys(), key=target_topic.__getitem__, reverse=True)\n",
    "    topics[target_id]['coeff'] = target_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "data_dict['topics'] = topics\n",
    "print(type(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['동업', '횡령죄', '결산', '경원', '김재윤', '카프', '조이']\n",
      "1 ['사기죄', '점유', '채권', '계약', '채권자', '채무', '소유권', '차용', '횡령죄', '변제', '재산', '양도', '담보', '임대', '강제집행']\n",
      "2 ['성폭력', '부착', '청소년', '명령', '아동', '신상', '강간', '추행', '불원', '치료', '송순', '기습', '특례법', '정보', '장애인']\n",
      "3 ['사고', '운전', '차량', '과실', '버스', '충돌', '폭력', '도로교통법', '충격', '도로', '신호', '운전자', '운행', '통행', '자동차']\n",
      "4 ['조합', '재건축', '사업', '정비', '총회', '청탁', '분양', '건설', '이한영', '비법', '당선자', '이사장', '선규', '주택조합', '조합원']\n",
      "5 ['회사', '주식', '대출', '자금', '재산', '배임', '업무', '주식회사', '회계', '경제범죄', '횡령', '조세', '손해', '은행', '거래']\n",
      "6 ['절도', '상습', '가중', '징역', '처벌', '절도죄', '야간', '특수', '주거침입', '확정판결', '지방법원', '조의', '미수', '특가법', '규정']\n",
      "7 ['쟁의', '노동조합', '집회', '노조', '근로자', '업무방해죄', '파업', '투쟁', '건조물', '옥외', '시위', '조합원', '점거', '노동', '공장']\n",
      "8 ['대법관', '형사소송법', '공소장', '징역', '상소', '재판', '적용', '송달', '규정', '처벌', '변경', '기간', '공판', '징역형', '대법원']\n",
      "9 ['위조', '공문서', '문서', '명의', '사문서', '변조', '행사', '제시', '사죄', '조제', '원본', '약속어음', '호적', '동행', '매매']\n",
      "10 ['공사', '건설', '수표', '어음', '지급', '약속어음', '계약', '산업', '공정', '안전', '사업', '주식회사', '작업', '입찰', '도급']\n",
      "11 ['항소', '일수', '부당', '강간', '징역', '요지', '징역형', '적용', '장애', '조제', '심신', '너무', '참작', '양형', '선택']\n",
      "12 ['매매', '토지', '매수', '계약', '대금', '부동산', '등기', '구암', '소유권', '임야', '지급', '손해', '농지', '개발', '사기죄']\n",
      "13 ['등기', '소유권', '부동산', '이전', '근저당권', '설정', '임야', '필지', '담보', '대지', '신탁', '금음', '가등기', '재산', '명의']\n",
      "14 ['감호', '보호감호', '보호', '사회', '재범', '동종', '청구인', '형기', '치료감호', '이상', '호의', '금고', '징역']\n",
      "15 ['사고', '도주', '조치', '구호', '교통사고', '운전자', '운전', '김석수', '김선영', '교통', '도로교통법', '옥희', '현장', '차량', '포터']\n",
      "16 ['법관', '소론', '기각', '산입', '일수', '대법관', '채증', '거기', '일영', '의견', '판결이유', '변호인', '본형', '후의', '양형']\n",
      "17 ['직무', '공무원', '공무', '집행', '뇌물', '수수', '방해', '규정', '알선', '뇌물죄', '체포', '변호', '이강', '정당', '정부']\n",
      "18 ['증언', '수사', '동법', '보험금', '구리시장', '한라일보', '유경', '절취', '정보원', '무기고', '고발', '공술', '치료', '서복', '도박']\n"
     ]
    }
   ],
   "source": [
    "# cluster마다 뽑히는 명사수가 달라 coef 순으로 정렬하여 top 10개 이런식으로 진행해야\n",
    "# number of topics to be shown\n",
    "n = 15\n",
    "\n",
    "for c_id in range(n_clusters):\n",
    "    if len(topics[c_id]['topic']) < n:\n",
    "        print(c_id, topics[c_id]['topic'])\n",
    "    else:\n",
    "        print(c_id, topics[c_id]['topic'][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"result.obj\", 'wb') as f:\n",
    "    pickle.dump(topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"result.obj\", 'rb') as f:\n",
    "    topics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
